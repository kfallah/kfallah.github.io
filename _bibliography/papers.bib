---
---

@article{
fallah2024manifold,
title={Manifold Contrastive Learning with Variational Lie Group Operators},
author={Kion Fallah and Alec Helbling and Kyle A. Johnsen and Christopher John Rozell},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=lVE1VeGQwg},
note={}
}

@inproceedings{pmlr-v162-fallah22a,
  title     = {Variational Sparse Coding with Learned Thresholding},
  author    = {Fallah, Kion and Rozell, Christopher J},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  pages     = {6034--6058},
  year      = {2022},
  volume    = {162},
  series    = {Proceedings of Machine Learning Research},
  month     = {17--23 Jul},
  publisher = {PMLR},
  pdf       = {https://proceedings.mlr.press/v162/fallah22a/fallah22a.pdf},
  url       = {https://proceedings.mlr.press/v162/fallah22a.html},
  abstract  = {Sparse coding strategies have been lauded for their parsimonious representations of data that leverage low dimensional structure. However, inference of these codes typically relies on an optimization procedure with poor computational scaling in high-dimensional problems. For example, sparse inference in the representations learned in the high-dimensional intermediary layers of deep neural networks (DNNs) requires an iterative minimization to be performed at each training step. As such, recent, quick methods in variational inference have been proposed to infer sparse codes by learning a distribution over the codes with a DNN. In this work, we propose a new approach to variational sparse coding that allows us to learn sparse distributions by thresholding samples, avoiding the use of problematic relaxations. We first evaluate and analyze our method by training a linear generator, showing that it has superior performance, statistical efficiency, and gradient estimation compared to other sparse distributions. We then compare to a standard variational autoencoder using a DNN generator on the CelebA dataset.}
}

@article{connor_learning_2021,
  title    = {Learning {Identity}-{Preserving} {Transformations} on {Data} {Manifolds}},
  url      = {http://arxiv.org/abs/2106.12096},
  abstract = {Many machine learning techniques incorporate identity-preserving transformations into their models to generalize their performance to previously unseen data. These transformations are typically selected from a set of functions that are known to maintain the identity of an input when applied (e.g., rotation, translation, flipping, and scaling). However, there are many natural variations that cannot be labeled for supervision or defined through examination of the data. As suggested by the manifold hypothesis, many of these natural variations live on or near a low-dimensional, nonlinear manifold. Several techniques represent manifold variations through a set of learned Lie group operators that define directions of motion on the manifold. However theses approaches are limited because they require transformation labels when training their models and they lack a method for determining which regions of the manifold are appropriate for applying each specific operator. We address these limitations by introducing a learning strategy that does not require transformation labels and developing a method that learns the local regions where each operator is likely to be used while preserving the identity of inputs. Experiments on MNIST and Fashion MNIST highlight our model's ability to learn identity-preserving transformations on multi-class datasets. Additionally, we train on CelebA to showcase our model's ability to learn semantically meaningful transformations on complex datasets in an unsupervised manner.},
  urldate  = {2022-04-01},
  journal  = {arXiv:2106.12096 [cs, stat]},
  author   = {Connor, Marissa and Fallah, Kion and Rozell, Christopher},
  month    = jun,
  year     = {2021},
  note     = {arXiv: 2106.12096},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file     = {arXiv Fulltext PDF:/Users/admin/Zotero/storage/YTPZX6LH/Connor et al. - 2021 - Learning Identity-Preserving Transformations on Da.pdf:application/pdf;arXiv.org Snapshot:/Users/admin/Zotero/storage/9KWT25WB/2106.html:text/html}
}

@inproceedings{fallah_learning_2020,
  title     = {Learning sparse codes from compressed representations with biologically plausible local wiring constraints},
  volume    = {33},
  url       = {https://proceedings.neurips.cc/paper/2020/file/a03fec24df877cc65c037673397ad5c0-Paper.pdf},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Fallah, Kion and Willats, Adam and Liu, Ninghao and Rozell, Christopher},
  year      = {2020},
  pages     = {13951--13963}
}